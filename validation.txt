RISK GOVERNANCE
Risk assessment and developing mitigation principles to manage risk is only likely to be effective where a coordinated and well communicated governance policy is put in place within the system being managed. Millstone et al. proposed three governance models:  Technocratic: where policy is directly informed by science and evidence from domain expertise.  Decisionistic: where risk evaluation and policy are developed using inputs beyond science alone. For instance, incorporating social and economic drivers.  Transparent(inclusive): where context for risk assessment is considered from the outset with input from science, politics, economics and civil society. This develops a model of pre-assessment that includes the views of wider stakeholders that shapes risk assessment and subsequent management policy.
There is a fine balance between the knowledge and findings of scientific experts, and perceptions of the lay public. While the technocratic approach may seem logical to some risk owners who work on the basis of reasoning using evidence, it is absolutely crucial for effective risk governance to include the wider stakeholder view. Rohrmann and Renns work on risk perception highlights some key reasons for this [20]. They identify four elements that influence the perception of risk: intuitive judgment associated with probabilities and damages; contextual factors surrounding the perceived characteristics of the risk (e.g., familiarity) and the risk situation (e.g., personal control); semantic associations linked to the risk source, people associated with the risk, and circumstances of the risk-taking situation; trust and credibility of the actors involved in the risk debate.
These factors are not particularly scientific, structured or evidence-based but, as noted by Fischoff et al., such forms of defining probabilities are countered by the strength of belief peoplehaveaboutthelikelihoodofanundesirableeventimpactingtheirownvalues. Ultimately, from a governance perspective, the more inclusive and transparent the policy development, the more likely the support and buy-in from the wider stakeholder group including lay people as well as operational staff for the risk management policies and principles.
There are several elements that are key to successful risk governance. Like much of the risk assessment process, there is no one-size solution for all endeavours. However, a major principle is ensuring that the governance activity (see below) is tightly coupled with everyday activity and decision-making. Cyber risk is as important as health and safety, financial processes, and human resources. These activities are now well established in decision-making. For instance, when hiring staff, the HR process is at the forefront of the recruiters activity. When travelling overseas, employees will always consult the financial constraints and processes for travel. Cyber security should be thought of in the same way a clear set of processes that reduce the risk of harm to individuals and the business. Everyone involved in the daily running of the system in question must understand that, for security to be effective, it must be part of everyday operational culture. The cyber risk governance approach is key to this cultural adoption.
Sasse and Flechais [22] identified human factors that can impact security governance, including people: having problems using security tools correctly; not understanding the importance of data, software, and systems for their organisation; not believing that the assets are at risk (i.e., that they would be attacked); or not understanding that their behaviour puts the system at risk. This highlights that risk cannot be mitigated with technology alone, and that concern assessment is important. If risk perception is such that there is a widely held view that people do not believe their assets will be attacked (as noted by [22]), despite statistics showing cyber security breaches are on the rise year-on-year, then there is likely to be a problem with the cyber security culture in the organisation. Educating people within an organisation is vital to ensuring cultural adoption of the principles defined in the risk management plan and associated security governance policy. People will generally follow the path of least resistance to get a job done or seek the path of highest reward. As Sasse and Flechais note, people fail to follow the required security behaviour for one of two reasons: (1) they are unable to behave as required (one example being that it is not technically possible to do so; another being that the security procedures and policies available to them are large, difficult to digest, or unclear) , (2) they do not want to behave in the way required (an example of this may be that they find it easier to work around the proposed low-risk but time consuming policy; another being that they disagree with the proposed policy).
Weirich and Sasse studied compliance with password rules as an example of compliance with security policy [23] and found that a lack of compliance was associated with people not believing that they were personally at risk and or that they would be held accountable for failure to follow security rules. There is thus a need to ensure a sense of responsibility and process for accountability, should there be a breach of policy. This must, of course, be mindful of legal and ethical implications, as well as the cultural issues around breaching rules, which is a balancing act. Risk communication, therefore, plays an important role in governance including aspects, such as:  Education: particularly around risk awareness and day-to-day handling of risks, including risk and concern assessment and management;  Training and inducement of behaviour change: taking the awareness provided by education and changing internal practices and processes to adhere to security policy;  Creation of confidence: both around organisational risk management and key individualsdevelop trust over time, and maintain this through strong performance and handling of risks.  Involvement: particularly in the risk decision-making process giving stakeholders an opportunity to take part in risk and concern assessment and partake in conflict resolution.
Dekkers principles on Just Culture [25] aim to balance accountability with learning in the context of security. He proposes the need to change the way in which we think about accountability so that it becomes compatible with learning and improving the security posture of an organisation. It is important that people feel able to report issues and concerns, particularly if they think they may be at fault. Accountability needs to be intrinsically linked to helping the organisation, without concern of being stigmatised and penalised. There is often an issue where those responsible for security governance have limited awareness and understanding of what it means to practise it in the operational world. In these cases there needs to be an awareness that there is possibly no clear right or wrong, and that poorly thought-out processes and practices are likely to have been behind the security breach, as opposed to malicious human behaviour. If this is the case, these need to be addressed and the person at fault needs to feel supported by their peers and free of anxiety. One suggestion Dekker makes is to have an independent team to handle security breach reports so people do not have to go through their line manager. If people are aware of the pathways and outcomes following security breaches it will reduce anxiety about what will happen and, therefore, lead to a more open security culture.
Given that security awareness and education is such an important factor in effective governance, Jaquith links security awareness with security metrics through a range of questions that may be considered as useful pointers for improving security culture:  Are employees acknowledging their security responsibilities as users of information systems? (Metric: % new employees completing security awareness training).  Are employees receiving training at intervals consistent with company policies? (Metric: %existing employees completing refresher training per policy).  Do security staff members possess sufficient skills and professional certifications? (Metric: % security staff with professional security certifications).  Are security staff members acquiring new skills at rates consistent with management objectives? (Metrics: # security skill mastered, average per employee and per security team member, fulfillment rate of target external security training workshops and classroom seminars).  Are security awareness and training efforts leading to measurable results? (Metrics: By business unit or office, correlation of password strength with the elapsed time since training classes; by business unit or office, correlation of tailgating rate with training latency).
Metrics may be a crude way to capture adherence to security policy, but when linked to questions that are related to the initial risk assessment, they can provide an objective and measurable way to continually monitor and report on the security of a system to the decision makers, as well as those responsible for its governance in an understandable and meaningful way. However, it is worth noting the complexity of metrics here with the use of the term acknowledging in the first bullet point. It does not necessarily mean the person will acknowledge their responsibilities merely by completing awareness training. This reinforces the points already made about the importance of human factors and security culture, and the following section on enacting security policy.
Overall, effective cyber risk governance will be underpinned by a clear and enactable security policy. This section focuses on the elements of risk assessment and management that are relevant to achieving this. From the initial phase of the risk assessment there should be a clear focus on the purpose and scope of the risk assessment exercise. During this phase, for more complex systems or whole system security, there should be a focus on identifying the objectives and goals of the system. These should be achievable with clear links from objectives to the processes that underpin them. Risks should be articulated as clear statements that capture the interdependencies between the vulnerabilities, threats, likelihoods and outcomes (e.g., causes and effects) that comprise the risk. Risk management decisions will be taken to mitigate threats identified for these processes, and these should be linked to the security policy, which will clearly articulate the required actions and activities taken (and by whom), often along with a clear timeline, to mitigate the risks. This should also include what is expected to happen as a consequence of this risk becoming a reality.
Presentation of risk assessment information in this context is important. Often heat maps and risk matrices are used to visualise the risks. However, research has identified limitations in the concept of combining multiple risk measurements (such as likelihood and impact) into a single matrix and heat map [30]. Attention should, therefore, be paid to the purpose of the visualisation and the accuracy of the evidence it represents for the goal of developing security policy decisions.
Human factors (see the Human Factors CyBOK Knowledge Area [27]), and security culture are fundamental to the enactment of the security policy. As discussed, people fail to follow the required security behaviour because they are unable to behave as required, or they do not want to behave in the way required [22]. A set of rules dictating how security risk management should operate will almost certainly fail unless the necessary actions are seen as linked to broader organisational governance, and therefore security policy, in the same way HR and finance policy requires. People must be enabled to operate in a secure way and not be the subject of a blame culture when things fail. It is highly likely that there will be security breaches, but the majority of these will not be intentional. Therefore, the security policy must be reflective and reactive to issues, responding to the Just Culture agenda and creating a policy of accountability for learning, and using mistakes to refine the security policy and underpinning processes not blame and penalise people.
Security education should be a formal part of all employees continual professional development, with reinforced messaging around why cyber security is important to the organisation, and the employees role and duties within this. Principles of risk communication are an important aspect of the human factor in security education. We have discussed the need for credible and trustworthy narratives and stakeholder engagement in the risk management process. There are additional principles to consider such as early and frequent communication, tailoring the message to the audience, pretesting the message and considering existing risk perceptions that should be part of the planning around security education.
Part of the final risk assessment and management outcomes should be a list of accepted risks with associated owners who have oversight for the organisational goals and assets underpinning the processes at risk. These individuals should be tightly coupled with review activity and should be clearly identifiable as responsible and accountable for risk management.
STAKEHOLDER ENGAGEMENT
From the research on human behaviour in cyber security over the past decade, one very clear theme has emerged: the importance of engaging in finding ways of making security work for employees. Communication and leadership are important in this regard. However, these aspects and others pertaining to organisational cultures are discussed in the Risk Management & Governance CyBOK Knowledge Area. Here, we focus on employees rather than organisational leadership and aspects, such as strategic board-level leadership of cyber security.
Lizzie Coles-Kemp and colleagues have developed an approach that takes employee involvement in improving security a step further. They use projective techniques (e.g., drawings and collages) to build representations of daily activity and ground the discussion of security in these. Case studies show how this helps to identify the root causes of insecure behaviour that the organisation sees as undesirable, in many cases badly designed security (echoing the results of Beautement et al.), but also more fundamental failings of the organisation to support the business and its individual tasks.
Creative security engagements (first mentioned by Dunphy et al.) encourage participants (employees in the company context or consumers or citizens in wider engagement) to reflect on:  their environment, the emotions they feel, the constraints they experience,  the pressures they are under, the actions and tasks they perform when generating and sharing information.
One particular technique for creative engagements using Lego for the physical modelling of information security threats was developed by the EU Trespass Project6 . This type of physical modelling bridges the gap between the typical diagrams (flow-charts and Unified Modelling Language (UML) diagrams, for example) with which security practitioners commonly work, and the everyday practices of the consumers who are affected by security design. Heath, Hall & Coles-Kemp reported a successful case study of this method to model security for a home banking application, which identified areas where human intervention and support needed to be provided to make security work overall.
Zurko & Simon pointed out that unusable security affects not only general employees who may not have specific computing or security education but also those who have significant technical skills, such as developers and system administrators. They also face increasing workloads and complexity and make mistakes because the libraries and application programming interfaces (APIs) they draw on are not usable. Arguably, errors that these technical users make generally have a more significant impact than mistakes made by general employees, e.g., the Heartbleed vulnerability.
Developers and password security : We noted above the usability issues of password and other authentication systems that have been studied extensively for end-users, highlighting problems and informing design decisions for better policies and motivating research into alternatives. However, end-users are not the only ones who have usability problems with passwords. The developers who are tasked with writing the code through which the passwords are stored must do so securely. Yet, history has shown that this complex task often fails due to human error with catastrophic results. If developers forget to hash and salt a password database, this can lead to millions of end-user passwords being compromised. Naiakshina et al. conducted a randomised control trial with computer science students, as well as freelance developers, and found that, similar to end-users, developers also suffer from task-focus and they see security as a secondary task. None of the student participants, and only a small number of freelance developers, implemented any kind of security unless explicitly prompted to do so. Interestingly, of those participants who did implement some security measures, the students did better than the freelance developers, who on the whole used more outdated and incorrect cryptographic mechanisms to store their passwords.
A number of studies, e.g., Enck et al. and Fahl et al. have highlighted the extent to which vulnerabilities manifest in modern eco-systems centred on app development. It was notable that, of the 96 developers who were contacted by Fahl et al., a large number were willing to provide information, but only 13 were interviewed because their companies refused permission for them to do so. From the interviews, Fahl et al. found that developers had little to no security training and were under extreme pressure to complete the app quicklyand that was the reason for the mistakes that led to vulnerabilities.
Acar et al. have studied the impact of online social networks, such as StackOverflow, on the security of code that developers produce. Two thirds of the developers who used StackOverflow or a textbook managed to produce a functionally correct solution within the allocated time, whereas only 40% of those using official documentation did. In terms of the security tasks, the results were reversed. Those using official documentation produced the most secure code and those using the StackOverflow the least. A traditional security response to this result would be use of StackOverflow should be forbidden. But clearly, the productivity price developers and their organisations would pay would be a hefty one. For instance, recent work has shown that developers utilise such forums to exchange information and offer mutual support for security problem-solving. That is not to say that such advice is always effective (as noted above) but the forums do provide a community of practice in which developers can share their problems and seek help. Banning such forums outright without replacing them with relevant support would, therefore, not address the crux of why developers seek such support.
The usability challenges of cryptographic APIs and their documentation have been highlighted by Arzt et al. and Nadi et al. , and tools proposed to support developers in their usage . Recently, tools have also been proposed to improve the usability of static analysis, e.g. . Green and Smith have synthesised insights from the existing body of research into a set of ten principles to make application programming interfaces for security and cryptography libraries more usable for developers . Patnaik et al.  identify four usability smells that indicate that cryptographic APIs may not be fully addressing such principles, offering insights to library developers on the key areas on which to focus in order to improve the usability of their libraries.
The disconnect between developers and users also needs to be considered. Caputo et al. highlighted those developers did not understand the impact of the lack of usability on individual performance and wellbeing, organisational productivity, or the effectiveness of security. They recommend that management must ensure that developers experience the results of the lack of security and usability directly  by having to deal with help desk calls, the impact of losses  and engage more. Recent work has provided insights into the role of strong organisational security cultures on developers mindsets with regards to security and how experts improve their security practices.
Humans and technologies do not exist in isolation. Humans conceive new technologies, design and implement them, and are also their users and maintainers. Cyber security is no different. Human behaviours shape cyber security (e.g., responses to phishing campaigns lead to anti-phishing filters or new security training). Equally, the design of cyber security (humans design those filters or training mechanisms) impacts peoples interactions with systems and the security mechanisms designed into those systems (e.g., impedance to primary tasks or increased workload arising from security tasks). We must consider this symbiotic relationship throughout the conception, design, implementation, maintenance, evolution  and lets not forget, decommissioning  of cyber security mechanisms. Human factors must play a central role as, after all, the purpose of cyber security is to protect people, their data, information and safety. We must  as far as possible  fit the task to the human and not the human to the task.
